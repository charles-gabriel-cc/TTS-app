version: '3.8'

services:
  # ServiÃ§o Ollama para modelos locais
  ollama:
    image: ollama/ollama:latest
    container_name: tts-ollama-dev
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    restart: unless-stopped
    networks:
      - tts-network
    # Suporte para GPU (requer NVIDIA Docker runtime)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ServiÃ§o Qdrant standalone (opcional - pode usar o integrado)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: tts-qdrant-dev
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    networks:
      - tts-network

  # AplicaÃ§Ã£o TTS principal - DESENVOLVIMENTO
  tts-app:
    build:
      context: .
      dockerfile: Dockerfile.dev  # Usa o Dockerfile de desenvolvimento
    container_name: tts-app-backend-dev
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      # CÃ“DIGO MONTADO COMO VOLUME (mudanÃ§as em tempo real)
      - .:/app
      # Excluir diretÃ³rios que nÃ£o devem ser montados
      - /app/tts-env
      - /app/__pycache__
      - /app/.gradio
      # Volumes para dados
      - ./logs:/app/logs
      - ./uploads:/app/uploads
      - ./data:/app/data
      - ./ccen-docentes:/app/ccen-docentes
    environment:
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8000
      - QDRANT_URL=http://qdrant:6333
      - OLLAMA_BASE_URL=http://ollama:11434
      - USE_LOCAL_COLLECTION=true
      - COLLECTION_NAME=ccen-docentes
      - USE_LOCAL_MODEL=true
      # ConfiguraÃ§Ãµes CUDA
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      # Desenvolvimento
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      # ConfiguraÃ§Ãµes Langsmith
      - LANGSMITH_TRACING=true
      - LANGSMITH_ENDPOINT=https://api.smith.langchain.com
      - LANGSMITH_PROJECT=backend
    depends_on:
      - qdrant
      - ollama
    networks:
      - tts-network
    # Suporte para GPU (requer NVIDIA Docker runtime)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Para desenvolvimento - manter container rodando
    tty: true
    stdin_open: true
    # Override do CMD para executar setup e depois bash interativo
    command: >
      bash -c "
      echo '=== TTS-APP DESENVOLVIMENTO com CUDA 11.8 e Python $$(python --version) ===';
      echo 'ğŸ”§ Verificando CUDA...';
      nvidia-smi;
      echo 'â³ Aguardando Qdrant ficar disponÃ­vel...';
      while ! curl -s http://qdrant:6333/health > /dev/null 2>&1; do
        echo '   Qdrant ainda nÃ£o disponÃ­vel, aguardando...';
        sleep 2;
      done;
      echo 'âœ… Qdrant estÃ¡ disponÃ­vel!';
      echo 'â³ Aguardando Ollama ficar disponÃ­vel...';
      while ! curl -s http://ollama:11434/api/tags > /dev/null 2>&1; do
        echo '   Ollama ainda nÃ£o disponÃ­vel, aguardando...';
        sleep 3;
      done;
      echo 'âœ… Ollama estÃ¡ disponÃ­vel!';
      echo 'ğŸ”¥ Aquecendo Ollama...';
      echo \"   Enviando mensagem inicial para carregar modelo: $${MODEL_NAME}...\";
      response=$$(curl -s -X POST http://ollama:11434/api/generate -H 'Content-Type: application/json' -d '{\"model\":\"$${MODEL_NAME}\",\"prompt\":\"Hello\",\"stream\":false}' | python3 -c 'import json,sys; data=json.load(sys.stdin); print(data.get(\"response\",\"Erro no parsing\").strip())' 2>/dev/null || echo 'Erro na conexÃ£o');
      echo 'ğŸš€ Ollama respondeu:' \"$$response\";
      echo '';
      echo 'ğŸ¯ AMBIENTE PRONTO PARA DESENVOLVIMENTO!';
      echo 'ğŸ“ Para iniciar o servidor Python, execute:';
      echo '   python server.py';
      echo '';
      echo 'ğŸ’¡ Ou use o container interativamente:';
      echo '   docker exec -it tts-app-backend-dev bash';
      echo '';
      echo 'ğŸ”„ Executando bash interativo...';
      bash
      "

volumes:
  qdrant_data:
    driver: local
  ollama_data:
    driver: local

networks:
  tts-network:
    driver: bridge 