# ğŸ Backend TTS-APP - Guia de Desenvolvimento

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10.11-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-latest-green.svg)
![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-orange.svg)
![Qdrant](https://img.shields.io/badge/Qdrant-Vector_DB-red.svg)

**Backend do Assistente Virtual CCEN - Ambiente de Desenvolvimento**

</div>

## ğŸ—ï¸ Arquitetura do Backend

Sistema simples com 3 componentes principais:

```
ğŸ’» FastAPI (server.py)
   â”œâ”€â”€ ğŸ¤ Whisper STT
   â”œâ”€â”€ ğŸ”Š Google TTS  
   â””â”€â”€ ğŸ’¬ Chat com IA
        â”œâ”€â”€ ğŸ¦™ Ollama (LLM)
        â”œâ”€â”€ ğŸ“Š Qdrant (Busca)
        â””â”€â”€ ğŸ“š Base CCEN
```

**Como funciona:**
1. **FastAPI** - API principal que recebe requisiÃ§Ãµes
2. **Ollama** - Modelo de IA local para conversas
3. **Qdrant** - Banco vetorial com dados dos professores

## ğŸ“ Estrutura de Arquivos

```
backend/
â”œâ”€â”€ ğŸš€ Scripts de InicializaÃ§Ã£o
â”‚   â”œâ”€â”€ AssistenteVirtualCCEN.bat     # Script principal (produÃ§Ã£o)
â”‚   â”œâ”€â”€ start_backend.bat/sh          # Desenvolvimento automÃ¡tico
â”‚   â”œâ”€â”€ start_backend_interactive.*   # Desenvolvimento interativo
â”‚   â””â”€â”€ start_backend_universal.bat   # Windows com detecÃ§Ã£o Docker
â”‚
â”œâ”€â”€ ğŸ³ Docker
â”‚   â”œâ”€â”€ docker-compose.yml            # ProduÃ§Ã£o
â”‚   â”œâ”€â”€ docker-compose.dev.yml        # Desenvolvimento
â”‚   â”œâ”€â”€ Dockerfile.dev                # Imagem desenvolvimento
â”‚   â””â”€â”€ .dockerignore
â”‚
â”œâ”€â”€ âš™ï¸ ConfiguraÃ§Ã£o
â”‚   â”œâ”€â”€ config.py                     # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”‚   
â”‚
â”œâ”€â”€ ğŸ”§ ServiÃ§os
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ chat_service.py           # ServiÃ§o principal de chat
â”‚   â”‚   â”œâ”€â”€ transcription_service.py  # Whisper STT
â”‚   â”‚   â””â”€â”€ embeddings.py             # Processamento embeddings
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ logger.py                 # Sistema de logs
â”‚   â”‚
â”‚   â”œâ”€â”€ server.py                     # Servidor FastAPI principal
â”‚   â”œâ”€â”€ app.py                        # Interface Gradio alternativa
â”‚   â””â”€â”€ embeddings.py                 # Script processamento docs
```

## ğŸš€ InÃ­cio RÃ¡pido

### 1ï¸âƒ£ **InicializaÃ§Ã£o AutomÃ¡tica (Recomendado)**

```batch
# Windows - ExecuÃ§Ã£o simples
.\AssistenteVirtualCCEN.bat

# Ou para desenvolvimento
.\start_backend.bat
```

### 2ï¸âƒ£ **InicializaÃ§Ã£o Manual**

```bash
# 1. Subir containers Docker
docker-compose -f docker-compose.dev.yml up -d

# 2. Entrar no container backend
docker exec -it tts-app-backend-dev bash

# 3. Executar servidor
python server.py
```

### 3ï¸âƒ£ **Verificar ServiÃ§os**

```bash
# Backend API
curl http://localhost:8000/health

# Ollama LLM
curl http://localhost:11434/api/tags

# Qdrant Vector DB
curl http://localhost:6333/health
```

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### ğŸ“ **Arquivo .env**

Crie o arquivo `.env` na pasta `backend/`:

```bash
# === CONFIGURAÃ‡Ã•ES DE IA ===
MODEL_NAME="qwen3:4b"              # Modelo Ollama
EMBED_MODEL="all-minilm:l6-v2"       # Modelo embeddings

# === QDRANT (BANCO VETORIAL) ===
QDRANT_URL="http://qdrant:6333"      # URL Qdrant
COLLECTION_NAME="ccen-docentes"      # Nome da coleÃ§Ã£o

# === OLLAMA (LLM LOCAL) ===
OLLAMA_BASE_URL="http://ollama:11434"

# === MONITORAMENTO (OPCIONAL) ===
LANGSMITH_TRACING="true"
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_PROJECT="backend"
LANGSMITH_API_KEY="sua-chave-aqui"   # Opcional

# === DESENVOLVIMENTO ===
SERVER_HOST="0.0.0.0"
SERVER_PORT="8000"
USE_LOCAL_MODEL="true"
USE_LOCAL_COLLECTION="true"
```

### ğŸ“š **Baixar Modelos IA**

```bash
# Conectar ao container Ollama
docker exec -it tts-ollama-dev bash

# Baixar modelo principal (escolha um)
ollama pull qwen3:4b         # Mais leve (4B parÃ¢metros)
ollama pull phi4:latest      # Equilibrado (7B parÃ¢metros)
ollama pull llama3.2:latest  # Alternativo

# Baixar modelo embeddings
ollama pull all-minilm:l6-v2

# Verificar modelos instalados
ollama list
```

## ğŸ”§ Scripts DisponÃ­veis

### ğŸ¯ **Para ProduÃ§Ã£o/Demo**
- `AssistenteVirtualCCEN.bat` - Inicia ambiente completo automaticamente

### ğŸ› ï¸ **Para Desenvolvimento**
- `start_backend.bat/.sh` - Desenvolvimento automÃ¡tico
- `start_backend_interactive.bat/.sh` - Desenvolvimento interativo (permite debug)
- `start_backend_universal.bat` - Windows com detecÃ§Ã£o automÃ¡tica Docker

### ğŸ“‹ **O que fazem os Scripts**

1. **Verificam** se Docker Desktop estÃ¡ rodando
2. **Param** containers anteriores (se houver)
3. **Sobem** todos os containers necessÃ¡rios
4. **Aguardam** inicializaÃ§Ã£o completa
5. **Executam** o servidor Python
6. **Mostram** logs em tempo real

## ğŸ¤– ServiÃ§os Principais

### ğŸ’¬ **ChatService** (`services/chat_service.py`)

ServiÃ§o principal responsÃ¡vel pela inteligÃªncia conversacional:

```python
# Funcionalidades principais:
- ğŸ§  IntegraÃ§Ã£o com Ollama (LLM local)
- ğŸ” Busca semÃ¢ntica no Qdrant
- ğŸ“š RAG (Retrieval-Augmented Generation)
- ğŸ¯ Ferramentas especializadas:
  â€¢ SearchQdrant - Busca geral
  â€¢ SearchTeacherInformation - Info especÃ­fica de professores
  â€¢ getTeacherNames - Lista de professores
  â€¢ SearchArticle - Busca em artigos cientÃ­ficos
```

### ğŸ¤ **TranscriptionService** (`services/transcription_service.py`)

Converte Ã¡udio em texto usando Whisper:

```python
# Funcionalidades:
- ğŸµ Suporte mÃºltiplos formatos (wav, mp3, m4a, etc.)
- ğŸŒ DetecÃ§Ã£o automÃ¡tica de idioma
- ğŸš€ Cache inteligente para otimizaÃ§Ã£o
- ğŸ”§ Modelos configurÃ¡veis (tiny â†’ large)
```

### ğŸ“Š **EmbeddingsService** (`services/embeddings.py`)

Processa documentos para busca vetorial:

```python
# Funcionalidades:
- ğŸ“„ Processamento PDFs
- âœ‚ï¸ DivisÃ£o semÃ¢ntica de textos
- ğŸ”¢ GeraÃ§Ã£o de embeddings
- ğŸ“š CriaÃ§Ã£o de coleÃ§Ãµes Qdrant
- ğŸ·ï¸ IndexaÃ§Ã£o com metadados
```

## ğŸ“š Base de Conhecimento

### ğŸ« **ColeÃ§Ã£o CCEN-Docentes**

ContÃ©m informaÃ§Ãµes dos professores do CCEN/UFPE:

```bash
# Estrutura da coleÃ§Ã£o:
- ğŸ‘¨â€ğŸ« nome_professor
- ğŸ¢ departamento  
- ğŸ†” id_lattes
- ğŸ“„ tipo_de_documento
- ğŸ“ text (conteÃºdo)
```

### ğŸ“– **Processamento de Novos Documentos**

```bash
# 1. Colocar PDFs na pasta ccen-docentes/
mkdir ccen-docentes
cp seus-pdfs.pdf ccen-docentes/

# 2. Executar processamento
docker exec -it tts-app-backend-dev python embeddings.py

# 3. Verificar coleÃ§Ã£o criada
curl http://localhost:6333/collections/ccen-docentes
```

## ğŸŒ API Endpoints

### ğŸ” **Endpoints Reais**

```http
### SaÃºde do Sistema
GET  /health                           # Status do backend

### Chat e IA
POST /chat/                           # Conversa bÃ¡sica com IA
POST /chat_with_tts/                  # Conversa com sÃ­ntese de voz

### Ãudio
POST /transcribe/                     # Speech-to-Text (Whisper)

### Cache e RecuperaÃ§Ã£o
GET  /pending_responses/{session_id}  # Respostas pendentes
```

### ğŸ“ **Modelos de RequisiÃ§Ã£o**

```python
# Modelo para chat
class ChatRequest(BaseModel):
    message: str
    session_id: str
```

### ğŸ“ **Exemplos de Uso**

```bash
# 1. Health Check
curl http://localhost:8000/health

# 2. Chat bÃ¡sico
curl -X POST http://localhost:8000/chat/ \
  -H "Content-Type: application/json" \
  -d '{"message": "Quem Ã© o professor JoÃ£o Silva?", "session_id": "user123"}'

# 3. Chat com TTS (retorna texto + Ã¡udio)
curl -X POST http://localhost:8000/chat_with_tts/ \
  -H "Content-Type: application/json" \
  -d '{"message": "Me fale sobre o CCEN", "session_id": "user123"}'

# 4. TranscriÃ§Ã£o de Ã¡udio
curl -X POST http://localhost:8000/transcribe/ \
  -F "file=@audio.wav"

# 5. Recuperar respostas pendentes
curl http://localhost:8000/pending_responses/user123
```

### ğŸ“‹ **Respostas da API**

```json
// GET /health
{
  "status": "healthy",
  "message": "Servidor estÃ¡ funcionando normalmente",
  "services": {
    "transcription": "ok",
    "chat": "ok", 
    "tts": "ok"
  }
}

// POST /chat/
{
  "response": "O professor JoÃ£o Silva Ã©..."
}

// POST /chat_with_tts/
{
  "text": "O CCEN Ã© o Centro de...",
  "audio": "base64_encoded_audio_data",
  "audio_format": "mp3"
}

// POST /transcribe/
{
  "text": "Texto transcrito do Ã¡udio"
}
```

## ğŸ› Debug e Desenvolvimento

### ğŸ“Š **Logs do Sistema**

```bash
# Logs do backend
docker-compose -f docker-compose.dev.yml logs -f tts-app

# Logs do Ollama
docker-compose -f docker-compose.dev.yml logs -f ollama

# Logs do Qdrant
docker-compose -f docker-compose.dev.yml logs -f qdrant

# Logs internos da aplicaÃ§Ã£o
tail -f logs/app.log
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### ğŸ›ï¸ **ParÃ¢metros do Modelo**

```python
# Em config.py
WHISPER_MODEL = "medium"        # tiny, small, medium, large
MODEL_NAME = "qwen3:4b"       # Modelo Ollama
EMBED_MODEL = "all-minilm:l6-v2" # Modelo embeddings
```

### ğŸŒ **ConfiguraÃ§Ãµes de Rede**

```yaml
# docker-compose.dev.yml
ports:
  - "8000:8000"    # FastAPI
  - "11434:11434"  # Ollama
  - "6333:6333"    # Qdrant HTTP
  - "6334:6334"    # Qdrant gRPC
```

### ğŸ“ **Volumes de Dados**

```yaml
volumes:
  - ./logs:/app/logs                    # Logs persistentes
  - ./uploads:/app/uploads              # Uploads persistentes  
  - ./ccen-docentes:/app/ccen-docentes  # Base de conhecimento
  - ollama_data:/root/.ollama           # Modelos Ollama
  - qdrant_data:/qdrant/storage         # Dados Qdrant
```

### ğŸ§ª **Testando MudanÃ§as**

```bash
# Restart rÃ¡pido apÃ³s mudanÃ§as
docker-compose restart tts-app

# Rebuild completo se necessÃ¡rio
docker-compose -f docker-compose.dev.yml up --build

# Teste de integraÃ§Ã£o
curl -X POST http://localhost:8000/chat/ \
  -H "Content-Type: application/json" \
  -d '{"message": "teste", "session_id": "test123"}'
```