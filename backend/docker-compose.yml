version: '3.8'

services:
  # ServiÃ§o Ollama para modelos locais
  ollama:
    image: ollama/ollama:latest
    container_name: tts-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    restart: unless-stopped
    networks:
      - tts-network
    # Suporte para GPU (requer NVIDIA Docker runtime)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ServiÃ§o Qdrant standalone (opcional - pode usar o integrado)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: tts-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    networks:
      - tts-network

  # AplicaÃ§Ã£o TTS principal
  tts-app:
    image: charlescc/backend-tts-app:latest
    container_name: tts-app-backend
    ports:
      - "8000:8000"
    volumes:
      # CÃ“DIGO MONTADO COMO VOLUME (mudanÃ§as em tempo real)
      - .:/app
      # Excluir diretÃ³rios que nÃ£o devem ser montados
      - /app/tts-env
      - /app/__pycache__
      - /app/.gradio
      # Volumes para dados
      - ./logs:/app/logs
      - ./uploads:/app/uploads
      - ./data:/app/data
      - ./ccen-docentes:/app/ccen-docentes
    environment:
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8000
      - QDRANT_URL=http://qdrant:6333
      - OLLAMA_BASE_URL=http://ollama:11434
      - USE_LOCAL_COLLECTION=true
      - COLLECTION_NAME=ccen-docentes
      - USE_LOCAL_MODEL=true
      - MODEL_NAME=qwen2.5:14b
      - EMBED_MODEL=all-minilm:l6-v2
      # ConfiguraÃ§Ãµes CUDA
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      #ConfiguraÃ§Ãµes Langsmith
      - LANGSMITH_TRACING=true
      - LANGSMITH_ENDPOINT=https://api.smith.langchain.com
      - LANGSMITH_PROJECT=backend
    depends_on:
      - qdrant
      - ollama
    restart: on-failure
    command: >
      bash -c "
      echo '=== TTS-APP com CUDA 11.8 e Python $$(python --version) ===';
      echo 'ğŸ”§ Verificando CUDA...';
      nvidia-smi;
      echo 'â³ Aguardando Qdrant ficar disponÃ­vel...';
      while ! curl -s http://qdrant:6333/health > /dev/null 2>&1; do
        echo '   Qdrant ainda nÃ£o disponÃ­vel, aguardando...';
        sleep 2;
      done;
      echo 'âœ… Qdrant estÃ¡ disponÃ­vel!';
      echo 'ğŸ”¥ Aquecendo Ollama...';
      echo '   Enviando mensagem inicial para carregar modelo...';
      response=$$(curl -s -X POST http://ollama:11434/api/generate -H 'Content-Type: application/json' -d '{\"model\":\"qwen2.5:14b\",\"prompt\":\"Hello\",\"stream\":false}' | python -c 'import json,sys; print(json.load(sys.stdin).get(\"response\",\"OK\"))' 2>/dev/null || echo 'Ollama aquecido');
      echo 'ğŸš€ Ollama respondeu:' $$response;
      echo '';
      echo 'ğŸ¯ AMBIENTE PRONTO!';
      echo 'ğŸ“ Para iniciar o servidor Python, execute:';
      echo '   python server.py';
      echo '';
      echo 'ğŸ’¡ Ou use o container interativamente:';
      echo '   docker exec -it tts-app-backend bash';
      echo '';
      echo 'ğŸ”„ Mantendo container rodando...';
      tail -f /dev/null
      "
    networks:
      - tts-network
    # Suporte para GPU (requer NVIDIA Docker runtime)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  qdrant_data:
    driver: local
  ollama_data:
    driver: local

networks:
  tts-network:
    driver: bridge 